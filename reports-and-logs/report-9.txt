In the last month, we have made the following progress in our project on privacy guardrails for question answering with large language models.

(1) We completed systematic evaluation of five distinct privacy-preserving methods on medical question answering, processing 250 out of 500 questions from the MedQA-USMLE dataset. Our phrase-level differential privacy approach achieved higher accuracy compared to non-private methods, while the InferDPT token-level approach showed significant accuracy loss. The non-private local-remote hybrid method achieved the highest accuracy, demonstrating the substantial value of external Chain of Thought for local models.

(2) We established that local model quality is a very important aspect of privacy-preserving system performance, with stronger local models achieving significantly higher accuracy than weaker ones. Our phrase-level differential privacy mechanism successfully generates diverse paraphrases while maintaining semantic coherence and providing effective privacy protection. We identified the optimal epsilon range for best privacy-utility balance in medical question answering scenarios.

(3) We validated our findings across multiple medical datasets including MedQA-USMLE, MedMCQA, and EMRQA-MSQUAD, ensuring our results generalize across different medical question styles and difficulty levels. We are now extending experiments to complete the remaining questions and cross-validate findings on additional datasets, while developing metrics such as entity exposure and attack success rates to further quantify the utility-privacy trade-off in sensitive medical domains.
