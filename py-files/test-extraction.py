import mauve
import json
import os
from imports_and_init import *
import re
from dp_sanitizer import get_embedding, load_sentence_bert, compute_similarity, differentially_private_replacement

# read from responses of my algorithm: 

# this function is more flexible

gpt2tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Load Sentence-BERT model
sbert_model = load_sentence_bert()

# using huggingface models
from huggingface_hub import login
login(token="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")


### okay, these are start and count. 
def get_tokens(text, start=0, count=50):
    """Extract tokens using GPT-2 tokenizer."""
    tokens = gpt2tokenizer.tokenize(text)
    selected_tokens = tokens[start:start+count]
    tokenized_string = gpt2tokenizer.convert_tokens_to_string(selected_tokens)
    return tokenized_string

# Function to count tokens
def count_tokens(text):
    """Return the number of GPT-2 tokens in the text."""
    return len(gpt2tokenizer.encode(text, add_special_tokens=False))

# def extract_refined_responses(
#     file_path, 
#     max_responses=2000):

#     # Check if file exists
#     if not os.path.exists(file_path):
#         raise FileNotFoundError(f"File {file_path} does not exist.")
    
#     # Read JSON file
#     with open(file_path, 'r', encoding='utf-8') as f:
#         try:
#             data = json.load(f)
#         except json.JSONDecodeError as e:
#             raise ValueError(f"Error decoding JSON from {file_path}: {e}")
    
#     # Extract refined_response fields, up to max_responses
#     refined_responses = [item.get("refined_response", "") for item in data[:max_responses]]
#     # refined_responses = [item.get("noisy_response", "") for item in data[:max_responses]]

    
    
#     # Filter out empty responses
#     refined_responses = [r for r in refined_responses if r.strip()]
    
#     return refined_responses

def extract_responses(file_path, max_responses=2000, field_name="refined_response"):
    # Check if file exists
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File {file_path} does not exist.")
    
    # Read JSON file
    with open(file_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Error decoding JSON from {file_path}: {e}")
    
    # Extract and clean responses
    refined_responses = [
        item[field_name]
        for item in data[:max_responses]
        if item[field_name].strip()
    ]
    
    return refined_responses


def slice_refined_text(text, prefix_text):
    """Extract refined text and remove editorial artifacts."""
    # Find the index where "Refined text:" appears
    refined_start_index = text.find("Refined text:")
    
    if refined_start_index != -1:
        # Slice the text starting after "Refined text:"
        refined_text = text[refined_start_index + len("Refined text:"):].strip()
    else:
        # Fallback: Use the entire text after the prompt if marker is missing
        refined_text = text.strip()

    # Remove prefix_text if it appears in the refined_text
    if refined_text.startswith(prefix_text):
        refined_text = refined_text[len(prefix_text):].strip()

    # Remove editorial artifacts (e.g., Teacher:, Note:, etc.)
    editorial_markers = [
        r'You are an AI assistant\..*?$',  # Remove "You are an AI assistant."
        r'Assistant:.*?$',  # Remove "Assistant:" and following text
        r'Teacher:.*?$',  # Remove "Teacher:" and following text
        r'Note:.*?$',  # Remove "Note:" and following text
        r'Edit:.*?$',  # Remove "Edit:" and following text
        r'Revision:.*?$',  # Remove "Revision:" and following text
        r'Alternative:.*?$',  # Remove "Alternative:" and following text
        r'Correction:.*?$',  # Remove "Correction:" and following text
        r'Improved version:.*?$',  # Remove "Improved version:" and following text
        r'\[.*?\].*?$',  # Remove bracketed instructional text (e.g., [INST])
        r'(\n|^)\s*[-*+#]\s.*?$',  # Remove bulleted or numbered lists that might be instructional
        r'(\n|^)\s*Generated by.*?$',  # Remove "Generated by" signatures
    ]
    for marker in editorial_markers:
        refined_text = re.sub(marker, '', refined_text, flags=re.IGNORECASE | re.MULTILINE)

    # Clean up trailing newlines, quotes, and extra spaces
    refined_text = re.sub(r'\n\s*\n\s*$', '', refined_text)
    refined_text = re.sub(r'^\s*["\']|["\']$', '', refined_text)
    return refined_text.strip()

# the only difference here is the post processing.
def adv_extraction_module(model, tokenizer, prefix_text, perturbed_generation):

    input_prompt = f'''
    Your taks is to refine 'Perturbed Generation' such that it is a seamless continuation of the 'Prefix Text'. 
    'Prefix Text': {prefix_text} \n 
    'Perturbed Generation': {perturbed_generation}\n
    Provide the refined text only. 
    Refined text: 
    '''

    # Tokenize the input prompt
    inputs = tokenizer(input_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        pad_token_id=tokenizer.eos_token_id,  # prevent warning
        do_sample=True,
        top_p=0.95,
        temperature=0.7
    )
    extended_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # print("extended_text = ", extended_text)

    refined_text = slice_refined_text(extended_text, prefix_text)

    return refined_text

### json based approach is not working well either.
def extract_json_response(output_str):
    start_marker = "JSON response:"
    end_marker = '"""'

    start_idx = output_str.find(start_marker)
    if start_idx == -1:
        raise ValueError("JSON response marker not found")

    # Move to the start of actual JSON content
    json_start = start_idx + len(start_marker)
    json_str = output_str[json_start:].strip()

    # Cut off after the closing triple quotes if it exists
    end_idx = json_str.find(end_marker)
    if end_idx != -1:
        json_str = json_str[:end_idx].strip()

    return json_str
def slice_json_at_closing_brace(output_str):
    end_idx = output_str.find("}") + 1  # include the closing brace
    if end_idx == 0:
        raise ValueError("No closing brace '}' found.")
    return output_str[:end_idx].strip()
def json_extraction_module(model, tokenizer, prefix_text, perturbed_generation):

    input_prompt = f'''
    Your task is to refine the 'perturbed_generation' so it is a seamless continuation of the 'prefix_text'.

    Respond ONLY in valid JSON format with the following structure:
    {{
    "refined_text": "<your refined continuation here>"
    }}

    Below is an example:

    'prefix_text': The weather today was unusually warm for early spring, with temperatures reaching nearly 80 degrees.
    'perturbed_generation': Many people decided to enjoy the sunshine by going for walks, sitting in parks, or grabbing ice cream from local shops.

    Expected JSON response:
    {{
    "refined_text": "Many people decided to enjoy the sunshine by going for walks, sitting in parks, or grabbing ice cream from local shops."
    }}

    Now refine the following:

    'prefix_text': {prefix_text}
    'perturbed_generation': {perturbed_generation}

    JSON response:
    '''


    # Tokenize the input prompt
    inputs = tokenizer(input_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_p=0.95,
        temperature=0.7
    )

    # Disables randomness. The model will choose the most likely token at each step (greedy decoding).


    extended_text = tokenizer.decode(outputs[0], skip_special_tokens=True)


    print("extended_text: ", extended_text)

    json_response = extract_json_response(extended_text)

    json_response = slice_json_at_closing_brace(json_response)
    # print("json obj: ", json_response)



    exit(0)
    # Try to extract the JSON safely
    match = re.search(r'\{.*\}', extended_text, re.DOTALL)
    if match:
        try:
            json_obj = json.loads(match.group())
            return json_obj.get("refined_text", "").strip()
        except json.JSONDecodeError:
            pass  # fallback or log as needed

    # fallback in case of failure
    return ""




def slice_refined_text_old(text, prefix_text):
    # Find the index where "Refined text:" appears
    refined_start_index = text.find("Refined Text:")
    
    if refined_start_index != -1:
        # Slice the text starting right after "Refined text:"
        refined_text = text[refined_start_index + len("Refined Text:"):].strip()

        # Check if prefix_text appears in the refined_text and remove it if it does
        if refined_text.startswith(prefix_text):
            refined_text = refined_text[len(prefix_text):].strip()

        return refined_text
    else:
        # If "Refined text:" is not found, return the original text
        return text

def extraction_module(model, tokenizer, prefix_text, perturbed_generation):

    # input_prompt = f'''
    # Your taks is to refine 'Perturbed Generation' such that it is a seamless continuation of the 'Prefix Text'. 
    # 'Prefix Text': {prefix_text} \n 
    # 'Perturbed Generation': {perturbed_generation}\n
    # Provide the refined text only. 
    # Refined text: 
    # '''

    # # one-shot prompting
    # input_prompt = f'''
    # Your task is to refine the 'Perturbed Generation' so that it seamlessly continues the 'Prefix Text' in tone, style, and content.

    # Example:
    # 'Prefix Text': (CNN)Eyewitness video showing white North Charleston police Officer Michael Slager shooting to death an unarmed black man has exposed discrepancies in the reports of the first officers on the scene. Slager has been fired and charged with murder in the death of 50
    # 'Perturbed Generation': -year-old Walter Scott. The footage, which was captured by a bystander, contradicts the initial police account that claimed Scott had taken the officer's Taser during a struggle. This incident has reignited national debates about police accountability and the use of excessive force, particularly against communities of color.
    # Refined Text: -year-old Walter Scott. A bystander's cell phone video, which began after an alleged struggle on the ground between Slager and Scott, shows the five-year police veteran shooting at Scott eight times as Scott runs away. Scott was hit five times. If words were exchanged between the men, they're are not audible on the tape. It's unclear what happened before Scott ran, or why he ran.

    # Input:
    # 'Prefix Text': {prefix_text}  
    # 'Perturbed Generation': {perturbed_generation}  
    # Provide the refined text only.  
    # Refined Text:
    # '''

    
    # two shot prompting
    input_prompt = f'''
    Your task is to refine the 'Perturbed Generation' so that it seamlessly continues the 'Prefix Text' in tone, style, and content. Ensure the refined text flows naturally, avoids unnecessary repetition, and maintains the narrative or thematic direction of the prefix.

    Example 1:
    'Prefix Text': (CNN)Eyewitness video showing white North Charleston police Officer Michael Slager shooting to death an unarmed black man has exposed discrepancies in the reports of the first officers on the scene. Slager has been fired and charged with murder in the death of 50
    'Perturbed Generation': -year-old Walter Scott. The footage, which was captured by a bystander, contradicts the initial police account that claimed Scott had taken the officer's Taser during a struggle. This incident has reignited national debates about police accountability and the use of excessive force, particularly against communities of color.
    Refined Text: -year-old Walter Scott. A bystander's cell phone video, which began after an alleged struggle on the ground between Slager and Scott, shows the five-year police veteran shooting at Scott eight times as Scott runs away. Scott was hit five times. If words were exchanged between the men, they're not audible on the tape. It's unclear what happened before Scott ran, or why he ran.

    Example 2:
    'Prefix Text': The old bookstore on Maple Street was a haven for bibliophiles, its shelves lined with dusty tomes and rare first editions. 
    'Perturbed Generation': The air smelled of paper and ink, but then it got weird with flickering lights and creepy noises. 
    Refined Text: The air smelled of aged paper and ink, creating a cozy atmosphere that invited readers to linger for hours among the creaking shelves. 

    Input:
    'Prefix Text': {prefix_text}  
    'Perturbed Generation': {perturbed_generation}  
    Provide the refined text only. 
    Refined Text:
    '''


    # Tokenize the input prompt
    inputs = tokenizer(input_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        pad_token_id=tokenizer.eos_token_id,  # prevent warning
        do_sample=True,
        top_p=0.95,
        temperature=0.7
    )
    extended_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("response = ", extended_text)

    # exit(1)

    refined_text = slice_refined_text_old(extended_text, prefix_text)

    return refined_text

if __name__ == "__main__":

    num_articles = 100
    
    # prepare the ground truth:
    print(f"Load CNN/DM dataset, first {num_articles}")
    ground_truths = []
    prompts = []
    dataset = load_dataset("cnn_dailymail", "3.0.0", split="test")

    # prepare the prompts and ground_truths
    for i in range(num_articles):
        article = dataset[i]["article"]
        prompt = get_tokens(article, 0, 50)
        ground_truth = get_tokens(article, 50, 100)
        prompts.append(prompt)
        ground_truths.append(ground_truth)


    model_name = "Qwen/Qwen2.5-7B-Instruct-1M"

    # model_name = "mistralai/Mistral-7B-Instruct-v0.2"

    print("loading local LM: ", model_name)
    qwen_tokenizer = AutoTokenizer.from_pretrained(model_name)
    qwen_model = AutoModelForCausalLM.from_pretrained(model_name, 
        device_map="auto"
    ) 



    for epsilon in [1, 2, 3, 4, 5]:
    # for epsilon in [1]:
        print("epsilon  = ", epsilon)
        # getting my results:



        # loading dataset.json
        # combined_dataset = []
        # with open("dataset.json", 'r', encoding='utf-8') as f:
        #     try:
        #         data = json.load(f)
        #     except json.JSONDecodeError as e:
        #         raise ValueError(f"Error decoding JSON from {file_path}: {e}")
        # # Add prefix_text to each item
        # for item in data:
        #     article_id = item.get("article_id")
        #     if article_id is None or not isinstance(article_id, int):
        #         print(f"Warning: Invalid or missing article_id in {file_path}, skipping item: {item}")
        #         continue
            
        #     # Get article from dataset
        #     if article_id >= len(dataset):
        #         print(f"Warning: article_id {article_id} exceeds dataset size, skipping...")
        #         continue
            
        #     article = dataset[article_id]["article"]
        #     prefix_text = get_tokens(article, 0, 50)
            
        #     # Create updated item
        #     updated_item = {
        #         "article_id": article_id,
        #         "prefix_text": prefix_text,
        #         "noisy_response": item.get("noisy_response", ""),
        #         "refined_response": item.get("refined_response", "")
        #     }
        #     combined_dataset.append(updated_item)


        # with open("updated_dataset.json", 'w', encoding='utf-8') as f:
        #     json.dump(combined_dataset, f, indent=4, ensure_ascii=False)
        # print(f"Updated dataset saved ") 

        # exit(1)     

        # directly from the LLM.
        noisy_res = extract_responses(
            file_path= f"./results/noisy_responses_{epsilon}_more_tokens.json", 
            field_name = "noisy_response", 
            max_responses =num_articles
        )

        # Qwen+ Hard prompting
        refined_res = extract_responses(
            file_path= f"./results/noisy_responses_{epsilon}_more_tokens.json", 
            field_name = "refined_response", 
            max_responses = num_articles
        )

        # apply the advanced extraction module:
        adv_refined = []
        for i in range(num_articles):

            article = dataset[i]["article"]

            prompt = prompts[i]

            ground_truth = ground_truths[i]

            noisy__ = noisy_res[i]

            refined__ = refined_res[i]

            ### apply advanced extraction
            adv_refined_res = extraction_module(qwen_model, 
                qwen_tokenizer, 
                prompt, 
                noisy__
            )

            adv_refined_res = get_tokens(adv_refined_res, 0, 100)


            adv_refined.append(adv_refined_res)

            prompt_tokens = count_tokens(prompt)
            mine_tokens = count_tokens(noisy__)
            rantext_tokens = count_tokens(refined__)

            adv_refined_res_tokens = count_tokens(adv_refined_res)

            ground_truth_tokens = count_tokens(ground_truth)


            print(f"{'Prompt':<12} = \033[35m{prompt}\033[0m (Tokens: {prompt_tokens})")
            print(f"{'noisy':<12} = \033[32m{noisy__}\033[0m (Tokens: {mine_tokens})")
            print(f"{'refined':<12} = \033[36m{refined__}\033[0m (Tokens: {rantext_tokens})")
            print(f"{'adv refined':<12} = \033[33m{adv_refined_res}\033[0m (Tokens: {adv_refined_res_tokens})")
            print(f"{'Ground Truth':<12} = \033[35m{ground_truth}\033[0m (Tokens: {ground_truth_tokens})")



        # # Compute Raw
        mauve_output = mauve.compute_mauve(
            p_text=noisy_res,
            q_text=ground_truths,
            # device_id=-1,  # Use CPU; change to 0 if you want to force GPU
            verbose=False  # Add this line to suppress progress bars
        )
        print(f"Noisy MAUVE Score: {mauve_output.mauve:.4f}")


        # # Compute MAUVE
        mauve_output = mauve.compute_mauve(
            p_text=refined_res,
            q_text=ground_truths,
            # device_id=-1,  # Use CPU; change to 0 if you want to force GPU
            verbose=False  # Add this line to suppress progress bars
        )
        print(f"Refined MAUVE Score: {mauve_output.mauve:.4f}")


        ## Compute Mauve for new refined
        mauve_output = mauve.compute_mauve(
            p_text=adv_refined,
            q_text=ground_truths,
            # device_id=-1,  # Use CPU; change to 0 if you want to force GPU
            verbose=False  # Add this line to suppress progress bars
        )
        print(f"ADV extraction MAUVE Score: {mauve_output.mauve:.4f}")
